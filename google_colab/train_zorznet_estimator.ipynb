{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de train_zorznet_estimator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Gp-sxdH_WdVL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Red ZorzNet CTC (Connectionist Temporal Classification)"
      ]
    },
    {
      "metadata": {
        "id": "JSPiEItPvyKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Puesta a punto"
      ]
    },
    {
      "metadata": {
        "id": "uiHlKPOlZHCS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Instalación de paquetes necesarios\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PtkPd7YoWZLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install python_speech_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kn5bgHjdWxSM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparación del entorno para poder correr mis módulos importados"
      ]
    },
    {
      "metadata": {
        "id": "yOE4tgZsW1et",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Tesis')\n",
        "sys.path.append('drive/My Drive/Tesis/repo')\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_1mN3rHXqLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "metadata": {
        "id": "eh1DAAWCXygf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importación de librerías necesarias"
      ]
    },
    {
      "metadata": {
        "id": "itKOqreUX1gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from src.neural_network.ZorzNet.ZorzNetData import ZorzNetData\n",
        "from src.neural_network.ZorzNet.ZorzNet import ZorzNet\n",
        "from src.utils.Database import Database\n",
        "from src.utils.ClassicLabel import ClassicLabel\n",
        "from src.utils.ProjectData import ProjectData\n",
        "import time\n",
        "from src.Estimators.zorznet.model_fn import model_fn\n",
        "from src.Estimators.zorznet.data_input_fn import data_input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K73_jDd7X_CO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Definición de los hiperparámetros de la red\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ROoJRMZwX-at",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_data = ProjectData()\n",
        "\n",
        "network_data = ZorzNetData()\n",
        "network_data.model_path = 'drive/My Drive/Tesis/repo/' + project_data.ZORZNET_MODEL_PATH\n",
        "network_data.checkpoint_path = 'drive/My Drive/Tesis/repo/' + project_data.ZORZNET_CHECKPOINT_PATH\n",
        "network_data.tensorboard_path = 'drive/My Drive/Tesis/repo/' + project_data.ZORZNET_TENSORBOARD_PATH\n",
        "\n",
        "network_data.num_classes = ClassicLabel.num_classes\n",
        "network_data.num_features = 494\n",
        "\n",
        "network_data.num_dense_layers_1 = 1\n",
        "network_data.num_units_1 = [400]\n",
        "network_data.dense_activations_1 = [tf.nn.relu] * network_data.num_dense_layers_1\n",
        "network_data.batch_normalization_1 = True\n",
        "network_data.keep_prob_1 = [0.6]\n",
        "network_data.kernel_init_1 = [tf.truncated_normal_initializer(mean=0, stddev=0.1)] * network_data.num_dense_layers_1\n",
        "network_data.bias_init_1 = [tf.zeros_initializer()] * network_data.num_dense_layers_1\n",
        "\n",
        "network_data.is_bidirectional = True\n",
        "# network_data.num_cell_units = [250]\n",
        "# network_data.cell_activation = [tf.nn.tanh]\n",
        "network_data.num_fw_cell_units = [256, 256]\n",
        "network_data.num_bw_cell_units = [256, 256]\n",
        "network_data.cell_fw_activation = [tf.nn.tanh] * 2\n",
        "network_data.cell_bw_activation = [tf.nn.tanh] * 2\n",
        "network_data.rnn_output_sizes = None\n",
        "\n",
        "network_data.num_dense_layers_2 = 2\n",
        "network_data.num_units_2 = [150, 100]\n",
        "network_data.dense_activations_2 = [tf.nn.relu] * network_data.num_dense_layers_2\n",
        "network_data.batch_normalization_2 = True\n",
        "network_data.keep_prob_2 = [0.6, 0.6]\n",
        "network_data.kernel_init_2 = [tf.truncated_normal_initializer(mean=0, stddev=0.1)] * network_data.num_dense_layers_2\n",
        "network_data.bias_init_2 = [tf.zeros_initializer()] * network_data.num_dense_layers_2\n",
        "\n",
        "network_data.dense_regularizer = 0.5\n",
        "network_data.rnn_regularizer = 0.5\n",
        "network_data.use_dropout = True\n",
        "\n",
        "network_data.decoder_function = tf.nn.ctc_greedy_decoder\n",
        "\n",
        "network_data.learning_rate = 0.001\n",
        "network_data.adam_epsilon = 0.0001\n",
        "network_data.optimizer = tf.train.AdamOptimizer(learning_rate=network_data.learning_rate, beta1=0.7, beta2=0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hi_FGeToYX5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configuración de entrenamiento"
      ]
    },
    {
      "metadata": {
        "id": "VnVktYkdYct3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_dir = 'drive/My Drive/Tesis/repo/out/zorznet/estimator/'\n",
        "\n",
        "base_path = 'drive/My Drive/Tesis/repo/data/tfrecords/librispeech/classic/'\n",
        "\n",
        "index_files = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "train_files = ['train_database_{}.tfrecords'.format(item) for item in index_files]\n",
        "val_files = ['test_database_1.tfrecords', 'test_database_2.tfrecords']\n",
        "test_files = ['test_database_2.tfrecords']\n",
        "\n",
        "train_files = list(map(lambda x: base_path + x, train_files))\n",
        "validate_files = list(map(lambda x: base_path + x, val_files))\n",
        "test_files = list(map(lambda x: base_path + x, test_files))\n",
        "\n",
        "train_batch_size = 100\n",
        "train_epochs = 1000\n",
        "\n",
        "validate_batch_size = 100\n",
        "               \n",
        "restore_run = True\n",
        "\n",
        "config = tf.estimator.RunConfig(\n",
        "    model_dir=model_dir,\n",
        "    save_checkpoints_steps=100,\n",
        "    save_summary_steps=100,\n",
        "    log_step_count_steps=100)\n",
        "\n",
        "\n",
        "model = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    params=network_data.as_dict(),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "if not restore_run:\n",
        "    shutil.rmtree(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q28RImgUvX60",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "metadata": {
        "id": "rYcvBQsDZe-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.train(\n",
        "    input_fn=lambda: data_input_fn(\n",
        "        filenames=train_files,\n",
        "        batch_size=train_batch_size,\n",
        "        parse_fn=Database.tfrecord_parse_dense_fn,\n",
        "        shuffle_buffer=10,\n",
        "        num_features=network_data.num_features,\n",
        "        num_epochs=1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fD4HmsIPvaLy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Validación\n"
      ]
    },
    {
      "metadata": {
        "id": "SyGxknhxvTJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(\n",
        "    input_fn=lambda: data_input_fn(\n",
        "        filenames=validate_files,\n",
        "        batch_size=validate_batch_size,\n",
        "        parse_fn=Database.tfrecord_parse_dense_fn,\n",
        "        shuffle_buffer=1,\n",
        "        num_features=network_data.num_features)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ebK_VwGpvkaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testeo"
      ]
    },
    {
      "metadata": {
        "id": "BcTE4yNhjtz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_tests = 10\n",
        "\n",
        "predictions = model.predict(\n",
        "    input_fn=lambda: data_input_fn(\n",
        "        filenames=test_files,\n",
        "        batch_size=1,\n",
        "        parse_fn=Database.tfrecord_parse_dense_fn,\n",
        "        shuffle_buffer=1,\n",
        "        num_features=network_data.num_features)\n",
        ")\n",
        "\n",
        "for item in predictions:\n",
        "  print(\"Predicted: \" + ClassicLabel.from_index(item))\n",
        "  num_tests -= 1\n",
        "  if num_tests == 0:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}